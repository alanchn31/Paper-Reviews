# ImageNet-trained CNNs are biased towards texture

Link to paper: https://arxiv.org/abs/1811.12231

## Overview:
CNNs are commonly thought to extract complex patterns from images, for example, examining edges and their orientations and generalizing towards shapes and texture. Recent research has however shown that texture plays an important role in image recognition. This paper tests this hypothesis by evaluating CNNs and human observers on images where texture and shape are conflicting. The results show that neural networks built on the usual imagenet training dataset with additional augmentations using style transfers can help neural networks generalize better, as the strong bias towards texture of images is shifted towards shapes of images.


## Write-up:
First question that may come into mind is that, should we care about the biases of our trained neural network if it is able to perform in the task as we intended? I think it depends on the problem that we are applying neural networks to in production. For example, if we are deploying neural network on a dataset that is not so diverse such as digit recognition with consistently the structure and background, then we do not need to worry about biasness as there is not much difference in the variations of data between training and production. In the real world, this is rarely the case, as in production, we need to account for dealing with images that have characteristics that are different from training set, for eg, in facial recognition, having faces in different backgrounds, settings or even sketches of faces. Hence we need to consider biases of the neural network to understand the generalizability of neural network so we can know in which cases neural networks may fail and some safeguards may be required (for example restrictions in UI to upload images of a certain format).

Next point of consideration is what type of bias should a neural network have? I think it depends on the context of the problem again in deciding what type of biases a neural network should have. Training a network to perform a task similar to how humans do it (such as medical diagnosis) and inducing a similar bias in the network can be desirable as a baseline but we should also, as much as possible, enable networks to generalize through techniques like data augmentation in tasks such as medical imaging, as the aim is to have networks that are more accurate than what humans.

How does a bias of the neural network change with the data we feed into it? As shown in the paper, as the network is trained on stylized images, the neural network may not perform as well at first (due to network being bias towards a certain aspect like texture), as the loss increases, the weights of the network gets adjusted to minimize the loss hence stylized images will change the bias towards other aspects of the images besides texture like shapes. This bias generalize better to datasets with corruptions because corruptions distorts textures of images. Hence, the shift in bias towards shape will help networks depend less on textures and more on shapes to distinguish images to their respective categories.

My personal takeaway from this paper is that it's important to understand how our model works in terms of the bias it may have. One way of doing so is to augment the test set (not just training set) to generate more testing data with techniques like style transfer, so we can further verify if the network is bias towards certain characteristics our training data has and whether our model will generalize.